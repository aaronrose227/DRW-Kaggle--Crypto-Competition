{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1 — Imports & Config\n\nimport os, gc, warnings\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection      import KFold\nfrom sklearn.preprocessing       import RobustScaler\nfrom sklearn.linear_model        import (\n    HuberRegressor, RANSACRegressor, TheilSenRegressor,\n    Lasso, ElasticNet, Ridge\n)\nfrom sklearn.cross_decomposition import PLSRegression\n\nfrom xgboost                     import XGBRegressor\nfrom lightgbm                    import LGBMRegressor\nfrom scipy.stats                 import pearsonr\n\nwarnings.filterwarnings(\"ignore\")\n\nclass Config:\n    # Paths\n    TRAIN_PATH   = \"/kaggle/input/drw-crypto-market-prediction/train.parquet\"\n    TEST_PATH    = \"/kaggle/input/drw-crypto-market-prediction/test.parquet\"\n    SUB_IN       = \"/kaggle/input/drw-crypto-market-prediction/sample_submission.csv\"\n    SUB_OUT_DIR  = \".\"\n\n    # Raw features you selected\n    RAW_FEATS    = [\n        \"X863\",\"X856\",\"X598\",\"X862\",\"X385\",\"X852\",\"X603\",\"X860\",\"X674\",\n        \"X415\",\"X345\",\"X855\",\"X174\",\"X302\",\"X178\",\"X168\",\"X612\",\n        \"buy_qty\",\"sell_qty\",\"volume\",\"X888\",\"X421\",\"X333\",\n        \"bid_qty\",\"ask_qty\"\n    ]\n\n    # Microstructure features\n    MICRO_FEATS  = [\n        \"volume_weighted_sell\", \"buy_sell_ratio\",\n        \"selling_pressure\",     \"effective_spread_proxy\"\n    ]\n    # Extra “robust” features\n    ROBUST_FEATS = [\n        \"log_volume\", \"bid_ask_imbalance\",\n        \"order_flow_imbalance\", \"liquidity_ratio\"\n    ]\n\n    # Full list used for modeling\n    ALL_FEATURES = RAW_FEATS + MICRO_FEATS + ROBUST_FEATS\n\n    LABEL_COL    = \"label\"\n    N_FOLDS      = 3\n    RANDOM_STATE = 42\n\n    # How much to up-weight recent data\n    DECAY_FACTOR = 0.9\n\n    # XGBoost GPU settings\n    XGB_PARAMS = {\n        \"tree_method\":      \"hist\",\n        \"predictor\":        \"gpu_predictor\",\n        \"gpu_id\":           0,\n        \"colsample_bylevel\":0.4778,\n        \"colsample_bynode\": 0.3628,\n        \"colsample_bytree\": 0.7107,\n        \"gamma\":            1.7095,\n        \"learning_rate\":    0.02213,\n        \"max_depth\":        20,\n        \"max_leaves\":       12,\n        \"min_child_weight\": 16,\n        \"n_estimators\":     1667,\n        \"subsample\":        0.06567,\n        \"reg_alpha\":        39.3524,\n        \"reg_lambda\":       75.4484,\n        \"verbosity\":        0,\n        \"random_state\":     RANDOM_STATE,\n        \"n_jobs\":           -1\n    }\n\n    # LightGBM GPU settings\n    LGBM_PARAMS = {\n        \"n_estimators\":      500,\n        \"learning_rate\":     0.03,\n        \"num_leaves\":        31,\n        \"min_child_samples\": 50,\n        \"subsample\":         0.8,\n        \"colsample_bytree\":  0.8,\n        \"reg_alpha\":         10,\n        \"reg_lambda\":        10,\n        \"device\":            \"gpu\",\n        \"verbosity\":         -1,\n        \"random_state\":      RANDOM_STATE,\n        \"n_jobs\":            -1\n    }\n\n    # All learners including a Ridge baseline\n    LEARNERS = [\n        {\"name\":\"xgb\",     \"Estimator\":XGBRegressor,   \"params\":XGB_PARAMS,  \"need_scale\":False},\n        {\"name\":\"lgbm\",    \"Estimator\":LGBMRegressor,  \"params\":LGBM_PARAMS, \"need_scale\":False},\n        {\"name\":\"ridge\",   \"Estimator\":Ridge,          \"params\":{},          \"need_scale\":True},\n        {\"name\":\"huber\",   \"Estimator\":HuberRegressor, \"params\":{\"epsilon\":1.5,\"alpha\":0.01,\"max_iter\":500},    \"need_scale\":True},\n        {\"name\":\"ransac\",  \"Estimator\":RANSACRegressor,\"params\":{\"min_samples\":0.7,\"max_trials\":100,\"random_state\":RANDOM_STATE},\"need_scale\":True},\n        {\"name\":\"theilsen\",\"Estimator\":TheilSenRegressor,\"params\":{\"max_subpopulation\":10000,\"random_state\":RANDOM_STATE},\"need_scale\":True},\n        {\"name\":\"lasso\",   \"Estimator\":Lasso,           \"params\":{\"alpha\":0.001,\"max_iter\":1000},                \"need_scale\":True},\n        {\"name\":\"elastic\",\"Estimator\":ElasticNet,       \"params\":{\"alpha\":0.001,\"l1_ratio\":0.5,\"max_iter\":1000}, \"need_scale\":True},\n        {\"name\":\"pls\",     \"Estimator\":PLSRegression,   \"params\":{\"n_components\":50},                          \"need_scale\":True},\n    ]\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-25T21:45:26.040789Z","iopub.execute_input":"2025-06-25T21:45:26.041121Z","iopub.status.idle":"2025-06-25T21:45:26.057141Z","shell.execute_reply.started":"2025-06-25T21:45:26.041101Z","shell.execute_reply":"2025-06-25T21:45:26.055867Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Cell 2 — Feature Engineering & Helpers\n\ndef feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n    # Microstructure\n    df[\"volume_weighted_sell\"]   = df[\"sell_qty\"] * df[\"volume\"]\n    df[\"buy_sell_ratio\"]         = df[\"buy_qty\"] / (df[\"sell_qty\"] + 1e-8)\n    df[\"selling_pressure\"]       = df[\"sell_qty\"] / (df[\"volume\"] + 1e-8)\n    df[\"effective_spread_proxy\"] = np.abs(df[\"buy_qty\"] - df[\"sell_qty\"]) / (df[\"volume\"] + 1e-8)\n\n    # Robust transforms\n    df[\"log_volume\"]             = np.log1p(df[\"volume\"])\n    df[\"bid_ask_imbalance\"]      = (df[\"bid_qty\"] - df[\"ask_qty\"]) / (df[\"bid_qty\"] + df[\"ask_qty\"] + 1e-8)\n    df[\"order_flow_imbalance\"]   = (df[\"buy_qty\"] - df[\"sell_qty\"]) / (df[\"buy_qty\"] + df[\"sell_qty\"] + 1e-8)\n    df[\"liquidity_ratio\"]        = (df[\"bid_qty\"] + df[\"ask_qty\"]) / (df[\"volume\"] + 1e-8)\n\n    # Replace infs/NaNs, then median‐impute\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    for c in df.columns:\n        if df[c].isna().any():\n            med = df[c].median()\n            df[c].fillna(med if not pd.isna(med) else 0, inplace=True)\n\n    return df\n\ndef create_time_decay_weights(n: int, decay: float=Config.DECAY_FACTOR) -> np.ndarray:\n    idx        = np.arange(n)\n    normalized = idx / (n - 1)\n    w          = decay ** (1.0 - normalized)\n    return w * n / w.sum()\n\ndef get_model_slices(n_samples: int):\n    return [\n        {\"name\":\"full_data\",  \"cutoff\":0},\n        {\"name\":\"last_75pct\",\"cutoff\":int(0.25*n_samples)},\n        {\"name\":\"last_50pct\",\"cutoff\":int(0.50*n_samples)},\n    ]\n\ndef train_single_model(X_tr, y_tr, X_val, y_val, X_test, learner, sw=None):\n    # scale if needed\n    if learner[\"need_scale\"]:\n        scaler = RobustScaler()\n        X_tr  = scaler.fit_transform(X_tr)\n        X_val = scaler.transform(X_val)\n        X_test= scaler.transform(X_test)\n\n    Model = learner[\"Estimator\"]\n    model = Model(**learner[\"params\"])\n\n    # pass sample_weight when supported\n    fit_kwargs = {}\n    if sw is not None and \"sample_weight\" in model.fit.__code__.co_varnames:\n        fit_kwargs[\"sample_weight\"] = sw\n\n    model.fit(X_tr, y_tr, **fit_kwargs)\n    return model.predict(X_val), model.predict(X_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T21:45:26.058636Z","iopub.execute_input":"2025-06-25T21:45:26.058982Z","iopub.status.idle":"2025-06-25T21:45:26.083279Z","shell.execute_reply.started":"2025-06-25T21:45:26.058952Z","shell.execute_reply":"2025-06-25T21:45:26.082344Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Cell 3 — Load Data (auto-detect paths), Drop Inf, Clip Labels\n\nimport os, glob\n\ndef load_data():\n    # 0) Auto-detect the dataset directory under /kaggle/input\n    dirs = glob.glob(\"/kaggle/input/*drw*crypto*\") or glob.glob(\"/kaggle/input/*drw-crypto*\") or glob.glob(\"/kaggle/input/*crypto*\")\n    if not dirs:\n        raise FileNotFoundError(\"Could not find the DRW crypto dataset under /kaggle/input\")\n    base = dirs[0]\n\n    # 1) Build the correct file paths\n    train_path = os.path.join(base, \"train.parquet\")\n    test_path  = os.path.join(base, \"test.parquet\")\n    sub_path   = os.path.join(base, \"sample_submission.csv\")\n\n    # 2) Read only the RAW_FEATS + label\n    train = pd.read_parquet(train_path, columns=Config.RAW_FEATS + [Config.LABEL_COL])\n    test  = pd.read_parquet(test_path,  columns=Config.RAW_FEATS)\n    sub   = pd.read_csv(sub_path)\n\n    # 3) Drop any RAW_FEATS that are entirely infinite\n    inf_train = train.isin([np.inf, -np.inf]).all()\n    drop_inf  = [c for c in Config.RAW_FEATS if inf_train.get(c, False)]\n    if drop_inf:\n        train.drop(columns=drop_inf, inplace=True)\n        test.drop(columns=drop_inf,  inplace=True)\n        Config.RAW_FEATS = [c for c in Config.RAW_FEATS if c not in drop_inf]\n\n    # 4) Feature engineering (adds MICRO_FEATS + ROBUST_FEATS)\n    train = feature_engineering(train)\n    test  = feature_engineering(test)\n\n    # 5) Prepare label (clip extremes) and remove from train\n    y = train[Config.LABEL_COL].clip(-10, 10).values\n    train.drop(columns=[Config.LABEL_COL], inplace=True)\n\n    # 6) Finalize the feature list\n    Config.ALL_FEATURES = Config.RAW_FEATS + Config.MICRO_FEATS + Config.ROBUST_FEATS\n\n    print(f\"Train shape: {train.shape}, Test shape: {test.shape}, Submission shape: {sub.shape}\")\n    return train.reset_index(drop=True), y, test.reset_index(drop=True), sub\n\n# Now call it:\ntrain_df, y, test_df, submission_df = load_data()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T21:45:26.084356Z","iopub.execute_input":"2025-06-25T21:45:26.084675Z","iopub.status.idle":"2025-06-25T21:45:26.126471Z","shell.execute_reply.started":"2025-06-25T21:45:26.084650Z","shell.execute_reply":"2025-06-25T21:45:26.125026Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1403497221.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Now call it:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmission_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/1403497221.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/*drw*crypto*\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/*drw-crypto*\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/*crypto*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not find the DRW crypto dataset under /kaggle/input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Could not find the DRW crypto dataset under /kaggle/input"],"ename":"FileNotFoundError","evalue":"Could not find the DRW crypto dataset under /kaggle/input","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"# Cell 4 — Train & OOF / Test Predictions\n\ndef train_and_evaluate(train_df, y, test_df):\n    n       = len(train_df)\n    slices  = get_model_slices(n)\n    feats   = Config.ALL_FEATURES\n\n    # storage\n    oof_preds  = {lr[\"name\"]:{s[\"name\"]:np.zeros(n) for s in slices} for lr in Config.LEARNERS}\n    test_preds = {lr[\"name\"]:{s[\"name\"]:np.zeros(len(test_df)) for s in slices} for lr in Config.LEARNERS}\n\n    decay_wts = create_time_decay_weights(n)\n    kf        = KFold(n_splits=Config.N_FOLDS, shuffle=False)\n\n    for fold, (tr_idx, val_idx) in enumerate(kf.split(train_df),1):\n        print(f\"\\n=== Fold {fold}/{Config.N_FOLDS} ===\")\n        X_val = train_df.iloc[val_idx][feats];  y_val = y[val_idx]\n        X_test= test_df[feats]\n\n        for sl in slices:\n            name, cut = sl[\"name\"], sl[\"cutoff\"]\n            sub       = train_df.iloc[cut:].reset_index(drop=True)\n            rel_idx   = tr_idx[tr_idx>=cut] - cut\n            if rel_idx.size==0: continue\n\n            X_tr = sub.iloc[rel_idx][feats]\n            y_tr = y[tr_idx[tr_idx>=cut]]\n            sw   = (create_time_decay_weights(len(sub))[rel_idx] \n                    if cut>0 else decay_wts[tr_idx])\n\n            print(f\"  Slice {name}: train size={len(X_tr)}\")\n\n            for learner in Config.LEARNERS:\n                try:\n                    val_p, test_p = train_single_model(\n                        X_tr, y_tr, X_val, y_val, X_test, learner, sw\n                    )\n                    # OOF\n                    mask = val_idx>=cut\n                    if mask.any():\n                        idxs = val_idx[mask]\n                        oof_preds[learner[\"name\"]][name][idxs] = val_p[mask]\n                    # carry-forward older rows\n                    if cut>0 and (~mask).any():\n                        oof_preds[learner[\"name\"]][name][val_idx[~mask]] = \\\n                          oof_preds[learner[\"name\"]][\"full_data\"][val_idx[~mask]]\n\n                    # test accumulation\n                    test_preds[learner[\"name\"]][name] += test_p\n\n                except Exception as e:\n                    print(f\"    ! {learner['name']} failed: {e}\")\n\n        gc.collect()\n\n    # normalize test preds\n    for lr in test_preds:\n        for sl in test_preds[lr]:\n            test_preds[lr][sl] /= Config.N_FOLDS\n\n    return oof_preds, test_preds, slices\n\noof_preds, test_preds, model_slices = train_and_evaluate(train_df, y, test_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T21:45:26.127135Z","iopub.status.idle":"2025-06-25T21:45:26.127401Z","shell.execute_reply.started":"2025-06-25T21:45:26.127283Z","shell.execute_reply":"2025-06-25T21:45:26.127295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5 — Ensemble & Submissions\n\ndef create_submissions(oof_preds, test_preds, submission_template):\n    subs = {}\n\n    # helper to average slice preds\n    def avg(pred_dict):\n        return np.mean(list(pred_dict.values()), axis=0)\n\n    # 1) per-learner simple & weighted\n    all_oof, all_test, scores = {}, {}, {}\n    for name in oof_preds:\n        o = avg(oof_preds[name]);  t = avg(test_preds[name])\n        r = pearsonr(y, o)[0]\n        if not np.isnan(r) and r>0:\n            scores[name]=r\n            all_oof[name]=o\n            all_test[name]=t\n            print(f\"{name:12s} OOF r = {r:.4f}\")\n    total = sum(scores.values())\n\n    # save individual\n    for name in all_test:\n        df = submission_template.copy()\n        df[\"prediction\"] = all_test[name]\n        df.to_csv(f\"{name}.csv\", index=False)\n        subs[name] = scores[name]\n\n    # weighted full ensemble\n    weights = {n:sc/total for n,sc in scores.items()}\n    w_oof = sum(weights[n]*all_oof[n] for n in weights)\n    w_tst = sum(weights[n]*all_test[n] for n in weights)\n    w_r   = pearsonr(y, w_oof)[0]\n    print(f\"weighted_ensemble OOF r = {w_r:.4f}, weights={weights}\")\n\n    df = submission_template.copy()\n    df[\"prediction\"] = w_tst\n    df.to_csv(\"submission_weighted.csv\", index=False)\n    subs[\"weighted_ensemble\"] = w_r\n\n    # simple full ensemble\n    s_tst = avg(all_test)\n    df = submission_template.copy()\n    df[\"prediction\"] = s_tst\n    df.to_csv(\"submission_simple.csv\", index=False)\n    subs[\"simple_ensemble\"] = pearsonr(y, avg(all_oof))[0]\n\n    return subs\n\n# load submission template\nsubmission_df = pd.read_csv(Config.SUB_IN)\nsubmission_scores = create_submissions(oof_preds, test_preds, submission_df)\nprint(\"\\nDone, scores:\", submission_scores)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T21:45:26.128738Z","iopub.status.idle":"2025-06-25T21:45:26.129174Z","shell.execute_reply.started":"2025-06-25T21:45:26.128953Z","shell.execute_reply":"2025-06-25T21:45:26.128971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6 — DONE\nprint(\"All submission CSVs written to\", Config.SUB_OUT_DIR) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T21:45:26.130812Z","iopub.status.idle":"2025-06-25T21:45:26.131176Z","shell.execute_reply.started":"2025-06-25T21:45:26.131025Z","shell.execute_reply":"2025-06-25T21:45:26.131037Z"}},"outputs":[],"execution_count":null}]}